{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%pip install scikit-learn",
   "id": "97723a1b226ab4d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%pip install xgboost",
   "id": "d5692f6d9fdfe667",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%pip install lightgbm",
   "id": "5f886e13907e7467",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-21T19:50:08.934544Z",
     "start_time": "2025-05-21T19:50:05.498698Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from xgboost import XGBRegressor, XGBRFRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Data",
   "id": "5c080e81006cbc36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T20:02:40.657295Z",
     "start_time": "2025-05-21T20:02:37.607259Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_parquet('../data/cayzn_train.parquet')",
   "id": "429127c17931ce64",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define preprocessing stages",
   "id": "3ce0a900802c64f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature selection",
   "id": "c70d1259fed66e36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T20:02:45.676734Z",
     "start_time": "2025-05-21T20:02:44.441775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Selection des variables\n",
    "\n",
    "# Comme dejà encodé, on ne vas pas utiliser ces features\n",
    "data = df.drop(columns=[\"sale_date\", \"departure_date\"])\n",
    "\n",
    "# On pourra éventuellement laissé de côté \"destination_public_holiday\" car constamment == 0\n",
    "\n",
    "data = data.drop(columns=[\"destination_current_public_holiday\"])\n"
   ],
   "id": "ca778e226ac7c90d",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature engineering",
   "id": "6fdff14f950c6bee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T12:37:37.017021Z",
     "start_time": "2025-05-22T12:37:33.412870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## On a observer une plus forte demande en mai (jours férié), juin (également), septembre(rentrée) et decembre (fêtes)\n",
    "# on peut créer une varaible \"haute saison\"\n",
    "data['haute_saison'] = data['sale_day_x'].apply(lambda x: 1 if x in [4, 5, 8, 11] else 0)\n",
    "\n",
    "data['is_public_holiday_near'] = (\n",
    "    (data[\"origin_days_to_next_public_holiday\"] <= 10)\n",
    ")\n",
    "\n",
    "# on peut regrouper les varaibles Od_number_of_similar_X_hours car elles sont très corrélé\n",
    "data['total_similar_trains'] = data['od_number_of_similar_2_hours'] + data['od_number_of_similar_4_hours'] + data['od_number_of_similar_12_hours']\n",
    "data['od_similar_low_density'] = data['total_similar_trains'] <= 8\n",
    "data['od_similar_medium_density'] = data['total_similar_trains'].between(8, 12, inclusive=\"right\")\n",
    "data['od_similar_high_density'] = data['total_similar_trains'] > 12\n",
    "## we dont encode the 'high' density to prevent from colinearity\n",
    "\n",
    "# également créer une feature \"is holiday\"\n",
    "data['is_public_holiday'] = (\n",
    "    (data['origin_current_school_holiday'] == 1) &\n",
    "    (data['destination_current_school_holiday'] == 1)\n",
    ")\n",
    "data.drop(columns=['origin_current_school_holiday', 'destination_current_school_holiday'], inplace=True)\n"
   ],
   "id": "1d4f2ee2def7835e",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T12:37:50.353414Z",
     "start_time": "2025-05-22T12:37:46.681687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# On encode uniquement les stations, car les date sont déjà encodés\n",
    "\n",
    "# Creation de la pipeline\n",
    "stationColumns = ['origin_station_name', 'destination_station_name']\n",
    "allStations = pd.concat([data[stationColumns[0]], data[stationColumns[1]]])\n",
    "stationEncoder = (sklearn.preprocessing.OneHotEncoder(sparse_output=False).fit(pd.DataFrame(allStations)))\n",
    "\n",
    "numericalColumns = data.drop(columns=[\"demand\"]).select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "preprocessor = sklearn.compose.ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('originEncoder', stationEncoder, [stationColumns[0]]),\n",
    "        ('destinationEncoder', stationEncoder, [stationColumns[1]]),\n",
    "        ('standardscaler', sklearn.preprocessing.StandardScaler(), numericalColumns)\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "RFRpipeline = sklearn.pipeline.Pipeline(\n",
    "    steps= [\n",
    "        ('preprocessing', preprocessor),\n",
    "        ('regressor', sklearn.ensemble.RandomForestRegressor(random_state=0, verbose=True, n_jobs=-1))\n",
    "    ]\n",
    ")\n",
    "\n",
    "XGBpipeline = sklearn.pipeline.Pipeline(\n",
    "    steps= [\n",
    "        ('preprocessing', preprocessor),\n",
    "        ('regressor', XGBRegressor(random_state=0, ))\n",
    "    ]\n",
    ")\n",
    "LightGBMpipeline = sklearn.pipeline.Pipeline( ## pertinent pour sa capacité à gérer les données données en très gros volumes, pas nécessaire de faire de la réduction de dimension car déjà le but de l'algo\n",
    "    steps= [\n",
    "        ('preprocessing', preprocessor),\n",
    "        ('regressor', LGBMRegressor(random_state=0))\n",
    "    ]\n",
    ")"
   ],
   "id": "c74e030d3d695ece",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prédiction",
   "id": "f23f42027a9dc122"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T12:37:57.759275Z",
     "start_time": "2025-05-22T12:37:55.315031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Création du jeu de test et de validation\n",
    "y = data.demand\n",
    "X = data.drop(columns=[\"demand\"])\n",
    "\n",
    "X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    train_size=0.7,\n",
    "    random_state=0,\n",
    "    \n",
    ")"
   ],
   "id": "f0660805abbe8992",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training with basic models and feature",
   "id": "7286963d422d2a13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# RANDOM FOREST REGRESSOR\n",
    "RFRpipeline.fit(X_train, y_train)\n",
    "print(RFRpipeline.score(X_val, y_val))"
   ],
   "id": "1ad20494d45e8cea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T20:03:32.597499Z",
     "start_time": "2025-05-21T20:03:16.701120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# XGBOOST REGRESSOR\n",
    "XGBpipeline.fit(X_train, y_train)\n",
    "print(XGBpipeline.score(X_val, y_val))"
   ],
   "id": "34ca96272992ecea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8162800073623657\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# LightGBM REGRESSOR\n",
    "LightGBMpipeline.fit(X_train, y_train)\n",
    "print(LightGBMpipeline.score(X_val, y_val))"
   ],
   "id": "c7cd5de4db5131f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Commentraire :\n",
    "* XGBoost, RFR, LightGBM ont des performances équivalentes, bien que XGBoost et LightGBM soient plus rapides à l'entrainement.\n",
    "* La sélection de variable à l'aide de l'importance (tant RFR que XGBoost) s'avère inefficace\n",
    "* Reste donc à utiliser la CV pour voir qui est véritablement le meilleur modèle, et trouver d'autre manière de sélectionner les variables, où d'en créer de nouvelles"
   ],
   "id": "bf98e129f2e35000"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Introduce Feature selection",
   "id": "6a0867f5c134c979"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Selection using RFR feature importance",
   "id": "9f135fccf4a931a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#compute feature importance\n",
    "importances = RFRpipeline.named_steps['regressor'].feature_importances_\n",
    "feature_names = RFRpipeline.named_steps['preprocessing'].get_feature_names_out()\n",
    "# Create a DataFrame for better visualization\n",
    "importances_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "# Display the feature importances\n",
    "importances_df"
   ],
   "id": "b5a3a14d3062e074",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "## Select a subset of features that gather a given % of the importance\n",
    "importanceRatio = 0.95\n",
    "cumulative_importance = importances_df['Importance'].cumsum()\n",
    "RFR_feature_subset = importances_df[cumulative_importance <= importanceRatio]['Feature'].tolist()\n",
    "RFR_feature_subset = [re.search(r'__(.+)$', feature).group(1) for feature in RFR_feature_subset]\n",
    "\n",
    "RFR_subset_numerical_columns = list(set(RFR_feature_subset) & set(numericalColumns))\n",
    "\n",
    "has_destination_station = False\n",
    "has_origin_station = False\n",
    "for feature in RFR_feature_subset:\n",
    "    if re.match(r\"(.*)origin_station_name_(.*)\", feature):\n",
    "        RFR_feature_subset.remove(feature)\n",
    "        if not has_origin_station:\n",
    "            RFR_feature_subset.append('origin_station_name')\n",
    "            has_origin_station = True\n",
    "    if re.match(r\"(.*)destination_station_name_(.*)\", feature):\n",
    "        RFR_feature_subset.remove(feature)\n",
    "        if not has_destination_station:\n",
    "            RFR_feature_subset.append('destination_station_name')\n",
    "            has_destination_station = True\n",
    "            \n",
    "print(f\"Selected features: {RFR_feature_subset}\")\n",
    "\n",
    "    "
   ],
   "id": "7940f3cfa55915fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Apply feature selection to the data\n",
    "\n",
    "X_rfr_subset = X[RFR_feature_subset]\n",
    "X_train_rfr_subset, X_val_rfr_subset, y_train_rfr_subset, y_val_rfr_subset = sklearn.model_selection.train_test_split(\n",
    "    X_rfr_subset,\n",
    "    y,\n",
    "    train_size=0.7,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "rfr_subset_preprocessor = sklearn.compose.ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('standardscaler', sklearn.preprocessing.StandardScaler(), RFR_subset_numerical_columns),\n",
    "        ('originEncoder', stationEncoder, [stationColumns[0]]) if 'origin_station_name' in RFR_feature_subset else ('originEncoder', 'passthrough', []),\n",
    "        ('destinationEncoder', stationEncoder, [stationColumns[1]]) if 'destination_station_name' in RFR_feature_subset else ('destinationEncoder', 'passthrough', [])\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "RFR_subset_pipeline = sklearn.pipeline.Pipeline(\n",
    "    steps= [\n",
    "        ('preprocessing', rfr_subset_preprocessor),\n",
    "        ('regressor', sklearn.ensemble.RandomForestRegressor(random_state=0, verbose=True, n_jobs=-1))\n",
    "    ]\n",
    ")\n",
    "XGB_subset_pipeline = sklearn.pipeline.Pipeline(\n",
    "    steps= [\n",
    "        ('preprocessing', rfr_subset_preprocessor),\n",
    "        ('regressor', XGBRegressor())\n",
    "    ]\n",
    ")"
   ],
   "id": "a114b7ee2545390f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prediction with RFR selected features",
   "id": "cc2d03e910a6b89a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# RANDOM FOREST REGRESSOR\n",
    "RFR_subset_pipeline.fit(X_train_rfr_subset, y_train_rfr_subset)\n",
    "print(RFR_subset_pipeline.score(X_val_rfr_subset, y_val_rfr_subset))"
   ],
   "id": "cdc66cc22839fd7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Prediction with XGBOOST selected features\n",
    "XGB_subset_pipeline.fit(X_train_rfr_subset, y_train_rfr_subset)\n",
    "print(XGB_subset_pipeline.score(X_val_rfr_subset, y_val_rfr_subset))"
   ],
   "id": "9c137541884ab013",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature selection with XGBoost",
   "id": "f32921ce8a6b7137"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T20:03:39.536634Z",
     "start_time": "2025-05-21T20:03:39.477908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Retrieve feature importance from XGBoost\n",
    "XGB_importance = XGBpipeline.named_steps['regressor'].feature_importances_\n",
    "XGB_features = XGBpipeline.named_steps['preprocessing'].get_feature_names_out()\n",
    "XGB_feature_importance = pd.DataFrame(\n",
    "    {\n",
    "        \"feature\" : XGB_features,\n",
    "        \"importance\" : XGB_importance\n",
    "     }\n",
    ").sort_values(by=\"importance\", ascending=False)\n",
    "XGB_feature_importance"
   ],
   "id": "4f15e79c51d236bc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                              feature  importance\n",
       "24             standardscaler__od_travel_time_minutes    0.299827\n",
       "31                         standardscaler__sale_day_x    0.192791\n",
       "33                          standardscaler__sale_week    0.047853\n",
       "23                     standardscaler__od_origin_year    0.039804\n",
       "20                     standardscaler__od_origin_time    0.039514\n",
       "29                              standardscaler__price    0.039051\n",
       "26      standardscaler__origin_current_school_holiday    0.028614\n",
       "18       standardscaler__od_number_of_similar_4_hours    0.023924\n",
       "37               standardscaler__total_similar_trains    0.021756\n",
       "15                standardscaler__od_destination_time    0.021713\n",
       "10   destinationEncoder__destination_station_name_cpe    0.021188\n",
       "16      standardscaler__od_number_of_similar_12_hours    0.019081\n",
       "5               originEncoder__origin_station_name_rb    0.018112\n",
       "4              originEncoder__origin_station_name_cpe    0.016027\n",
       "21                     standardscaler__od_origin_week    0.013387\n",
       "1               originEncoder__origin_station_name_bb    0.013064\n",
       "14  standardscaler__destination_days_to_next_schoo...    0.012823\n",
       "22                  standardscaler__od_origin_weekday    0.012724\n",
       "0               originEncoder__origin_station_name_ag    0.012106\n",
       "19                    standardscaler__od_origin_month    0.011654\n",
       "32                         standardscaler__sale_month    0.009805\n",
       "39               remainder__od_similar_medium_density    0.009771\n",
       "13  standardscaler__destination_days_to_next_publi...    0.008815\n",
       "28  standardscaler__origin_days_to_next_school_hol...    0.008659\n",
       "35                          standardscaler__sale_year    0.008076\n",
       "17       standardscaler__od_number_of_similar_2_hours    0.007758\n",
       "6     destinationEncoder__destination_station_name_ag    0.007527\n",
       "30                           standardscaler__sale_day    0.006727\n",
       "11    destinationEncoder__destination_station_name_rb    0.005645\n",
       "8    destinationEncoder__destination_station_name_cdm    0.005618\n",
       "7     destinationEncoder__destination_station_name_bb    0.005501\n",
       "12  standardscaler__destination_current_school_hol...    0.005419\n",
       "2              originEncoder__origin_station_name_cdm    0.004821\n",
       "3              originEncoder__origin_station_name_cgm    0.000679\n",
       "9    destinationEncoder__destination_station_name_cgm    0.000165\n",
       "25      standardscaler__origin_current_public_holiday    0.000000\n",
       "27  standardscaler__origin_days_to_next_public_hol...    0.000000\n",
       "36                       standardscaler__haute_saison    0.000000\n",
       "34                       standardscaler__sale_weekday    0.000000\n",
       "38                  remainder__od_similar_low_density    0.000000\n",
       "40                 remainder__od_similar_high_density    0.000000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>standardscaler__od_travel_time_minutes</td>\n",
       "      <td>0.299827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>standardscaler__sale_day_x</td>\n",
       "      <td>0.192791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>standardscaler__sale_week</td>\n",
       "      <td>0.047853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>standardscaler__od_origin_year</td>\n",
       "      <td>0.039804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>standardscaler__od_origin_time</td>\n",
       "      <td>0.039514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>standardscaler__price</td>\n",
       "      <td>0.039051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>standardscaler__origin_current_school_holiday</td>\n",
       "      <td>0.028614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>standardscaler__od_number_of_similar_4_hours</td>\n",
       "      <td>0.023924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>standardscaler__total_similar_trains</td>\n",
       "      <td>0.021756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>standardscaler__od_destination_time</td>\n",
       "      <td>0.021713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>destinationEncoder__destination_station_name_cpe</td>\n",
       "      <td>0.021188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>standardscaler__od_number_of_similar_12_hours</td>\n",
       "      <td>0.019081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>originEncoder__origin_station_name_rb</td>\n",
       "      <td>0.018112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>originEncoder__origin_station_name_cpe</td>\n",
       "      <td>0.016027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>standardscaler__od_origin_week</td>\n",
       "      <td>0.013387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>originEncoder__origin_station_name_bb</td>\n",
       "      <td>0.013064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>standardscaler__destination_days_to_next_schoo...</td>\n",
       "      <td>0.012823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>standardscaler__od_origin_weekday</td>\n",
       "      <td>0.012724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>originEncoder__origin_station_name_ag</td>\n",
       "      <td>0.012106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>standardscaler__od_origin_month</td>\n",
       "      <td>0.011654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>standardscaler__sale_month</td>\n",
       "      <td>0.009805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>remainder__od_similar_medium_density</td>\n",
       "      <td>0.009771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>standardscaler__destination_days_to_next_publi...</td>\n",
       "      <td>0.008815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>standardscaler__origin_days_to_next_school_hol...</td>\n",
       "      <td>0.008659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>standardscaler__sale_year</td>\n",
       "      <td>0.008076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>standardscaler__od_number_of_similar_2_hours</td>\n",
       "      <td>0.007758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>destinationEncoder__destination_station_name_ag</td>\n",
       "      <td>0.007527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>standardscaler__sale_day</td>\n",
       "      <td>0.006727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>destinationEncoder__destination_station_name_rb</td>\n",
       "      <td>0.005645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>destinationEncoder__destination_station_name_cdm</td>\n",
       "      <td>0.005618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>destinationEncoder__destination_station_name_bb</td>\n",
       "      <td>0.005501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>standardscaler__destination_current_school_hol...</td>\n",
       "      <td>0.005419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>originEncoder__origin_station_name_cdm</td>\n",
       "      <td>0.004821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>originEncoder__origin_station_name_cgm</td>\n",
       "      <td>0.000679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>destinationEncoder__destination_station_name_cgm</td>\n",
       "      <td>0.000165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>standardscaler__origin_current_public_holiday</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>standardscaler__origin_days_to_next_public_hol...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>standardscaler__haute_saison</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>standardscaler__sale_weekday</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>remainder__od_similar_low_density</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>remainder__od_similar_high_density</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T20:05:19.181719Z",
     "start_time": "2025-05-21T20:05:19.151898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "## Select a subset of features that gather a given % of the importance\n",
    "importanceRatio = 0.98\n",
    "XGB_cumulative_importance = XGB_feature_importance['importance'].cumsum()\n",
    "XGB_feature_subset = XGB_feature_importance[XGB_cumulative_importance <= importanceRatio]['feature'].tolist()\n",
    "XGB_feature_subset = [re.search(r'__(.+)$', feature).group(1) for feature in XGB_feature_subset]\n",
    "\n",
    "XGB_subset_numerical_Columns = list(set(XGB_feature_subset) & set(numericalColumns))\n",
    "\n",
    "# Remove the station name features and add the original feature names\n",
    "\n",
    "has_destination_station = False\n",
    "has_origin_station = False\n",
    "for feature in XGB_feature_subset:\n",
    "    if re.match(r\"(.*)origin_station_name_(.*)\", feature):\n",
    "        XGB_feature_subset.remove(feature)\n",
    "        if not has_origin_station:\n",
    "            XGB_feature_subset.append('origin_station_name')\n",
    "            has_origin_station = True\n",
    "            \n",
    "for feature in XGB_feature_subset:\n",
    "    if re.match(r\"(.*)destination_station_name_(.*)\", feature):\n",
    "        XGB_feature_subset.remove(feature)\n",
    "        if not has_destination_station:\n",
    "            XGB_feature_subset.append('destination_station_name')\n",
    "            has_destination_station = True\n",
    "            \n",
    "print(f\"Selected features: {XGB_feature_subset}\")"
   ],
   "id": "baedf65d0d8f76d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['od_travel_time_minutes', 'sale_day_x', 'sale_week', 'od_origin_year', 'od_origin_time', 'price', 'origin_current_school_holiday', 'od_number_of_similar_4_hours', 'total_similar_trains', 'od_destination_time', 'od_number_of_similar_12_hours', 'origin_station_name_cpe', 'od_origin_week', 'destination_days_to_next_school_holiday', 'od_origin_weekday', 'od_origin_month', 'sale_month', 'od_similar_medium_density', 'destination_days_to_next_public_holiday', 'origin_days_to_next_school_holiday', 'sale_year', 'od_number_of_similar_2_hours', 'sale_day', 'origin_station_name', 'destination_station_name']\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T20:05:30.111422Z",
     "start_time": "2025-05-21T20:05:29.970602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Apply feature selection to the data\n",
    "X_XGB_subset = X[XGB_feature_subset]\n",
    "X_train_XGB_subset, X_val_XGB_subset, y_train_XGB_subset, y_val_XGB_subset = sklearn.model_selection.train_test_split(\n",
    "    X_XGB_subset,\n",
    "    y,\n",
    "    train_size=0.7,\n",
    "    random_state=0\n",
    ")\n",
    "XGB_subset_preprocessor = sklearn.compose.ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('standardscaler', sklearn.preprocessing.StandardScaler(), XGB_subset_numerical_Columns),\n",
    "        ('originEncoder', stationEncoder, [stationColumns[0]]) if 'origin_station_name' in XGB_feature_subset else ('originEncoder', 'passthrough', []),\n",
    "        ('destinationEncoder', stationEncoder, [stationColumns[1]]) if 'destination_station_name' in XGB_feature_subset else ('destinationEncoder', 'passthrough', [])\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "XGB_XGB_subset_pipeline = sklearn.pipeline.Pipeline(\n",
    "    steps= [\n",
    "        ('preprocessing', XGB_subset_preprocessor),\n",
    "        ('regressor', XGBRegressor())\n",
    "    ]\n",
    ")\n",
    "RFR_XGB_subset_pipeline = sklearn.pipeline.Pipeline(\n",
    "    steps= [\n",
    "        ('preprocessing', XGB_subset_preprocessor),\n",
    "        ('regressor', sklearn.ensemble.RandomForestRegressor(random_state=0, verbose=True, n_jobs=-1, n_estimators=25))\n",
    "    ]\n",
    ")\n",
    "\n",
    "LigthtGBM_XGB_subset_pipeline = sklearn.pipeline.Pipeline(\n",
    "    steps= [\n",
    "        ('preprocessing', XGB_subset_preprocessor),\n",
    "        ('regressor', LGBMRegressor())\n",
    "    ]\n",
    ")"
   ],
   "id": "58a5414fb27efe28",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['origin_station_name_cpe'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m## Apply feature selection to the data\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m X_XGB_subset \u001B[38;5;241m=\u001B[39m \u001B[43mX\u001B[49m\u001B[43m[\u001B[49m\u001B[43mXGB_feature_subset\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m      3\u001B[0m X_train_XGB_subset, X_val_XGB_subset, y_train_XGB_subset, y_val_XGB_subset \u001B[38;5;241m=\u001B[39m sklearn\u001B[38;5;241m.\u001B[39mmodel_selection\u001B[38;5;241m.\u001B[39mtrain_test_split(\n\u001B[0;32m      4\u001B[0m     X_XGB_subset,\n\u001B[0;32m      5\u001B[0m     y,\n\u001B[0;32m      6\u001B[0m     train_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.7\u001B[39m,\n\u001B[0;32m      7\u001B[0m     random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m\n\u001B[0;32m      8\u001B[0m )\n\u001B[0;32m      9\u001B[0m XGB_subset_preprocessor \u001B[38;5;241m=\u001B[39m sklearn\u001B[38;5;241m.\u001B[39mcompose\u001B[38;5;241m.\u001B[39mColumnTransformer(\n\u001B[0;32m     10\u001B[0m     transformers \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     11\u001B[0m         (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstandardscaler\u001B[39m\u001B[38;5;124m'\u001B[39m, sklearn\u001B[38;5;241m.\u001B[39mpreprocessing\u001B[38;5;241m.\u001B[39mStandardScaler(), XGB_subset_numerical_Columns),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     15\u001B[0m     remainder\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpassthrough\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     16\u001B[0m )\n",
      "File \u001B[1;32m~\\PycharmProjects\\wiremind_2\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:4108\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   4106\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_iterator(key):\n\u001B[0;32m   4107\u001B[0m         key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(key)\n\u001B[1;32m-> 4108\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_indexer_strict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcolumns\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m   4110\u001B[0m \u001B[38;5;66;03m# take() does not accept boolean indexers\u001B[39;00m\n\u001B[0;32m   4111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(indexer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\wiremind_2\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001B[0m, in \u001B[0;36mIndex._get_indexer_strict\u001B[1;34m(self, key, axis_name)\u001B[0m\n\u001B[0;32m   6197\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   6198\u001B[0m     keyarr, indexer, new_indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reindex_non_unique(keyarr)\n\u001B[1;32m-> 6200\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raise_if_missing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeyarr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   6202\u001B[0m keyarr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtake(indexer)\n\u001B[0;32m   6203\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Index):\n\u001B[0;32m   6204\u001B[0m     \u001B[38;5;66;03m# GH 42790 - Preserve name from an Index\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\wiremind_2\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001B[0m, in \u001B[0;36mIndex._raise_if_missing\u001B[1;34m(self, key, indexer, axis_name)\u001B[0m\n\u001B[0;32m   6249\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNone of [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] are in the [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maxis_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   6251\u001B[0m not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(ensure_index(key)[missing_mask\u001B[38;5;241m.\u001B[39mnonzero()[\u001B[38;5;241m0\u001B[39m]]\u001B[38;5;241m.\u001B[39munique())\n\u001B[1;32m-> 6252\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnot_found\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not in index\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mKeyError\u001B[0m: \"['origin_station_name_cpe'] not in index\""
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prediction with XGBoost selected features",
   "id": "b08e4e5ab30cced"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# RANDOM FOREST REGRESSOR\n",
    "RFR_XGB_subset_pipeline.fit(X_train_XGB_subset, y_train_XGB_subset)\n",
    "print(RFR_XGB_subset_pipeline.score(X_val_XGB_subset, y_val_XGB_subset))"
   ],
   "id": "34100947ae051df2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# XGBOOST REGRESSOR\n",
    "XGB_subset_pipeline.fit(X_train_XGB_subset, y_train_XGB_subset)\n",
    "print(XGB_subset_pipeline.score(X_val_XGB_subset, y_val_XGB_subset))"
   ],
   "id": "9d46988598f21de8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Autre pistes de feature engineering",
   "id": "426f855e924207e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Comparaison des modèles en CV",
   "id": "10c7e06136224223"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T12:43:52.937155Z",
     "start_time": "2025-05-22T12:38:13.337117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xgb_param_grid = {\n",
    "    \n",
    "}\n",
    "\n",
    "lgbm_param_grid = {\n",
    "}\n",
    "\n",
    "xgb_search = GridSearchCV(XGBpipeline, xgb_param_grid, cv=10, n_jobs=-1, verbose=1)\n",
    "lgbm_search = GridSearchCV(LightGBMpipeline, lgbm_param_grid, cv=10, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit on your data (replace df and target column as needed)\n",
    "\n",
    "xgb_search.fit(X, y)\n",
    "lgbm_search.fit(X, y)\n",
    "\n",
    "print(\"XGBoost best RMSE:\", xgb_search.best_score_)\n",
    "print(\"XGBoost best params:\", xgb_search.best_params_)\n",
    "\n",
    "print(\"LightGBM best RMSE:\", lgbm_search.best_score_)\n",
    "print(\"LightGBM best params:\", lgbm_search.best_params_)\n",
    "\n"
   ],
   "id": "5115d95bed065503",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.158167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1082\n",
      "[LightGBM] [Info] Number of data points in the train set: 632841, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 3.000149\n",
      "XGBoost best RMSE: 0.7309605121612549\n",
      "XGBoost best params: {}\n",
      "LightGBM best RMSE: 0.750381616416935\n",
      "LightGBM best params: {}\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f395221039a5076",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## test des réseaux de neuronnes",
   "id": "6d59511c6e2e06ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Definition de la structure",
   "id": "62b68653b2598aa6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DemandPredictor(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DemandPredictor, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),       # 1er bloc\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(128, 64),              # 2e bloc\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(64, 32),               # 3e bloc\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(32, 1)                 # Sortie\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "model = DemandPredictor(input_dim=X.shape[1])\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n"
   ],
   "id": "3a98e10b3a758d69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## NN training loop",
   "id": "de92425f9af90482"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y_val",
   "id": "b39465b10d1742b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Split train/val\n",
    "X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "y_val = y_val.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop = True)\n",
    "\n",
    "# Preprocessing\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_val = preprocessor.transform(X_val)\n",
    "# Tensors\n",
    "train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "val_dataset = TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Instancier le modèle\n",
    "model = DemandPredictor(input_dim=X_train.shape[1])\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "# Boucle d'entraînement\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} | Train Loss: {train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f}\")\n"
   ],
   "id": "9a36eee33ec3d1a9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
